{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edadbc7a-45f7-454e-af68-3e26e534ce92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cae05bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+---------+-------+-------+--------+-----+------------+\n",
      "|sample|chromosome|position|reference|variant|quality|genotype|depth|allele_depth|\n",
      "+------+----------+--------+---------+-------+-------+--------+-----+------------+\n",
      "| w1118|     chr2L|     223|        A|      .|28.2394|     0/0|    0|           0|\n",
      "| w1118|     chr2L|     224|        T|      .|28.2394|     0/0|    0|           0|\n",
      "| w1118|     chr2L|     225|        A|      .|28.2394|     0/0|    0|           0|\n",
      "| w1118|     chr2L|     226|        T|      .|28.2394|     0/0|    0|           0|\n",
      "| w1118|     chr2L|     226|        T|      .|31.7802|     0/0|    2|           0|\n",
      "| w1118|     chr2L|     227|        A|      .| 34.543|     0/0|    2|           2|\n",
      "| w1118|     chr2L|     228|        T|      .| 34.543|     0/0|    2|           2|\n",
      "| w1118|     chr2L|     229|        G|      .| 34.543|     0/0|    2|           2|\n",
      "| w1118|     chr2L|     230|        A|      .| 34.543|     0/0|    2|           2|\n",
      "| w1118|     chr2L|     231|        T|      .| 34.543|     0/0|    2|           2|\n",
      "| w1118|     chr2L|     232|        C|      .| 34.543|     0/0|    2|           2|\n",
      "| w1118|     chr2L|     233|        G|      .| 34.543|     0/0|    2|           2|\n",
      "| w1118|     chr2L|     234|        C|      .| 34.543|     0/0|    2|           2|\n",
      "| w1118|     chr2L|     235|        G|      .| 34.543|     0/0|    2|           2|\n",
      "| w1118|     chr2L|     236|        T|      .| 34.543|     0/0|    2|           2|\n",
      "| w1118|     chr2L|     237|        A|      .| 34.543|     0/0|    2|           2|\n",
      "| w1118|     chr2L|     238|        T|      .| 34.543|     0/0|    2|           2|\n",
      "| w1118|     chr2L|     239|        G|      .| 34.543|     0/0|    2|           2|\n",
      "| w1118|     chr2L|     240|        C|      .| 34.543|     0/0|    2|           2|\n",
      "| w1118|     chr2L|     241|        G|      .| 34.543|     0/0|    2|           2|\n",
      "+------+----------+--------+---------+-------+-------+--------+-----+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ref_parent_file = \"../../data/parquets/w1118.parquet\"\n",
    "alt_parent_file = \"../../data/parquets/oregonr.parquet\"\n",
    "sample_file_glob = \"../../data/parquets/WT-G0-*.parquet\"\n",
    "ref_parent_name = \"w1118\"\n",
    "alt_parent_name = \"oregonr\"\n",
    "\n",
    "ref_qual_cutoff = 200\n",
    "ref_depth_cutoff = 30\n",
    "\n",
    "ref_out_file = \"../../data/parquets/reference.parquet\"\n",
    "sample_out_file = \"../../data/parquets/progeny.parquet\"\n",
    "\n",
    "df = spark.read.parquet(ref_parent_file, alt_parent_file)\n",
    "\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb53b281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------+-------+--------+-----+------------+---------+-------+-----------+\n",
      "|sample|chromosome|int_pos|quality|genotype|depth|allele_depth|reference|variant|mod_variant|\n",
      "+------+----------+-------+-------+--------+-----+------------+---------+-------+-----------+\n",
      "| w1118|     chr2L|    223|28.2394|     0/0|    0|           0|        A|      .|          A|\n",
      "| w1118|     chr2L|    224|28.2394|     0/0|    0|           0|        T|      .|          T|\n",
      "| w1118|     chr2L|    225|28.2394|     0/0|    0|           0|        A|      .|          A|\n",
      "| w1118|     chr2L|    226|28.2394|     0/0|    0|           0|        T|      .|          T|\n",
      "| w1118|     chr2L|    226|31.7802|     0/0|    2|           0|        T|      .|          T|\n",
      "| w1118|     chr2L|    227| 34.543|     0/0|    2|           2|        A|      .|          A|\n",
      "| w1118|     chr2L|    228| 34.543|     0/0|    2|           2|        T|      .|          T|\n",
      "| w1118|     chr2L|    229| 34.543|     0/0|    2|           2|        G|      .|          G|\n",
      "| w1118|     chr2L|    230| 34.543|     0/0|    2|           2|        A|      .|          A|\n",
      "| w1118|     chr2L|    231| 34.543|     0/0|    2|           2|        T|      .|          T|\n",
      "| w1118|     chr2L|    232| 34.543|     0/0|    2|           2|        C|      .|          C|\n",
      "| w1118|     chr2L|    233| 34.543|     0/0|    2|           2|        G|      .|          G|\n",
      "| w1118|     chr2L|    234| 34.543|     0/0|    2|           2|        C|      .|          C|\n",
      "| w1118|     chr2L|    235| 34.543|     0/0|    2|           2|        G|      .|          G|\n",
      "| w1118|     chr2L|    236| 34.543|     0/0|    2|           2|        T|      .|          T|\n",
      "| w1118|     chr2L|    237| 34.543|     0/0|    2|           2|        A|      .|          A|\n",
      "| w1118|     chr2L|    238| 34.543|     0/0|    2|           2|        T|      .|          T|\n",
      "| w1118|     chr2L|    239| 34.543|     0/0|    2|           2|        G|      .|          G|\n",
      "| w1118|     chr2L|    240| 34.543|     0/0|    2|           2|        C|      .|          C|\n",
      "| w1118|     chr2L|    241| 34.543|     0/0|    2|           2|        G|      .|          G|\n",
      "+------+----------+-------+-------+--------+-----+------------+---------+-------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parents = spark.sql(\n",
    "    \"SELECT sample, chromosome, CAST(position AS INTEGER) AS int_pos, quality, genotype, depth, allele_depth, reference, variant, (CASE WHEN variant='.' THEN reference ELSE variant END) AS mod_variant FROM df\"\n",
    ")\n",
    "parents.createOrReplaceTempView(\"parents\")\n",
    "\n",
    "parents.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b54014f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/nas/longleaf/apps/spark/2.4.0/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1159, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/nas/longleaf/apps/spark/2.4.0/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 985, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/nas/longleaf/apps/spark/2.4.0/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1164, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o25.sql",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a63f0799e492>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \"\"\"SELECT chromosome, int_pos, COUNT(variant) AS n_variants FROM parents\n\u001b[1;32m      3\u001b[0m     \u001b[0mGROUP\u001b[0m \u001b[0mBY\u001b[0m \u001b[0mchromosome\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_pos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     HAVING n_variants >= 2\"\"\"\n\u001b[0m\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[0mcheck_unique_variants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"check_unique_variants\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nas/longleaf/apps/spark/2.4.0/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nas/longleaf/apps/spark/2.4.0/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nas/longleaf/apps/spark/2.4.0/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nas/longleaf/apps/spark/2.4.0/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    334\u001b[0m             raise Py4JError(\n\u001b[1;32m    335\u001b[0m                 \u001b[0;34m\"An error occurred while calling {0}{1}{2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 format(target_id, \".\", name))\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o25.sql"
     ]
    }
   ],
   "source": [
    "check_unique_variants = spark.sql(\n",
    "    \"\"\"SELECT chromosome, int_pos, COUNT(variant) AS n_variants FROM parents\n",
    "    GROUP BY chromosome, int_pos\n",
    "    HAVING n_variants >= 2\"\"\"\n",
    ")\n",
    "check_unique_variants.createOrReplaceTempView(\"check_unique_variants\")\n",
    "\n",
    "check_unique_variants.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5d432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_ref_query = \"\"\"SELECT\n",
    "    sample,\n",
    "    parents.chromosome,\n",
    "    parents.int_pos,\n",
    "    reference,\n",
    "    mod_variant,\n",
    "    quality,\n",
    "    genotype,\n",
    "    depth,\n",
    "    allele_depth\n",
    "    FROM parents\n",
    "    INNER JOIN check_unique_variants ON (\n",
    "        parents.chromosome = check_unique_variants.chromosome\n",
    "        AND parents.int_pos = check_unique_variants.int_pos\n",
    "        )\n",
    "    WHERE genotype!='0/1'\n",
    "        AND quality > {ref_qual_cutoff}\n",
    "        AND LENGTH(mod_variant) <= 1\n",
    "        AND LENGTH(reference) <= 1\n",
    "        AND depth > {ref_depth_cutoff}\"\"\"\n",
    "\n",
    "    temp_ref = spark.sql(\n",
    "        temp_ref_query,\n",
    "        ref_qual_cutoff=ref_qual_cutoff,\n",
    "        ref_depth_cutoff=ref_depth_cutoff\n",
    "        )\n",
    "    temp_ref.createOrReplaceTempView(\"temp_ref\")\n",
    "    \n",
    "    temp_ref.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c78df371",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'bytes' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      4\u001b[0m     spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVCF DB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n",
      "File \u001b[0;32m/nas/longleaf/apps/spark/2.4.0/spark-2.4.0-bin-hadoop2.7/python/pyspark/__init__.py:51\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkConf\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkContext\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrdd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RDD, RDDBarrier\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfiles\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkFiles\n",
      "File \u001b[0;32m/nas/longleaf/apps/spark/2.4.0/spark-2.4.0-bin-hadoop2.7/python/pyspark/context.py:31\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtempfile\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NamedTemporaryFile\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Py4JError\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accumulators\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maccumulators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Accumulator\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbroadcast\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Broadcast, BroadcastPickleRegistry\n",
      "File \u001b[0;32m/nas/longleaf/apps/spark/2.4.0/spark-2.4.0-bin-hadoop2.7/python/pyspark/accumulators.py:97\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msocketserver\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mSocketServer\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserializers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m read_int, PickleSerializer\n\u001b[1;32m    100\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccumulator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccumulatorParam\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    103\u001b[0m pickleSer \u001b[38;5;241m=\u001b[39m PickleSerializer()\n",
      "File \u001b[0;32m/nas/longleaf/apps/spark/2.4.0/spark-2.4.0-bin-hadoop2.7/python/pyspark/serializers.py:71\u001b[0m\n\u001b[1;32m     68\u001b[0m     protocol \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     69\u001b[0m     xrange \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cloudpickle\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _exception_message\n\u001b[1;32m     75\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPickleSerializer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMarshalSerializer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUTF8Deserializer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/nas/longleaf/apps/spark/2.4.0/spark-2.4.0-bin-hadoop2.7/python/pyspark/cloudpickle.py:145\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m types\u001b[38;5;241m.\u001b[39mCodeType(\n\u001b[1;32m    127\u001b[0m             co\u001b[38;5;241m.\u001b[39mco_argcount,\n\u001b[1;32m    128\u001b[0m             co\u001b[38;5;241m.\u001b[39mco_kwonlyargcount,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m             (),\n\u001b[1;32m    142\u001b[0m         )\n\u001b[0;32m--> 145\u001b[0m _cell_set_template_code \u001b[38;5;241m=\u001b[39m \u001b[43m_make_cell_set_template_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcell_set\u001b[39m(cell, value):\n\u001b[1;32m    149\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Set the value of a closure cell.\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/nas/longleaf/apps/spark/2.4.0/spark-2.4.0-bin-hadoop2.7/python/pyspark/cloudpickle.py:126\u001b[0m, in \u001b[0;36m_make_cell_set_template_code\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m types\u001b[38;5;241m.\u001b[39mCodeType(\n\u001b[1;32m    110\u001b[0m         co\u001b[38;5;241m.\u001b[39mco_argcount,\n\u001b[1;32m    111\u001b[0m         co\u001b[38;5;241m.\u001b[39mco_nlocals,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m         (),\n\u001b[1;32m    124\u001b[0m     )\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCodeType\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_argcount\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_kwonlyargcount\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_nlocals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_stacksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_flags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_consts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_varnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_firstlineno\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_lnotab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_cellvars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# this is the trickery\u001b[39;49;00m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'bytes' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "    parent_query = \"\"\"SELECT\n",
    "    sample,\n",
    "    chromosome,\n",
    "    int_pos,\n",
    "    mod_variant AS {allele}\n",
    "    FROM temp_ref\n",
    "    WHERE sample={parent_name}\"\"\"\n",
    "\n",
    "    ref_parent = spark.sql(\n",
    "        ref_query,\n",
    "        allele=\"ref_allele\",\n",
    "        parent_name=ref_parent_name\n",
    "    )\n",
    "    ref_parent.createOrReplaceTempView(\"ref_parent\")\n",
    "\n",
    "    alt_parent = spark.sql(\n",
    "        ref_query,\n",
    "        allele=\"alt_allele\",\n",
    "        parent_name=alt_parent_name\n",
    "    )\n",
    "    alt_parent.createOrReplaceTempView(\"alt_parent\")\n",
    "\n",
    "    ref = spark.sql(\"\"\"SELECT * FROM ref_parent\n",
    "        FULL JOIN alt_parent ON (\n",
    "            ref_parent.chromosome = alt_parent.chromosome\n",
    "            AND ref_parent.int_pos = alt_parent.int_pos\n",
    "            )\n",
    "        ORDER BY ref_parent.chromosome, ref_parent.int_pos\"\"\"\n",
    "    )\n",
    "    ref.createOrReplaceTempView(\"ref\")\n",
    "\n",
    "    ref_out = spark.sql(\n",
    "        \"\"\"SELECT chromosome, int_pos AS position, ref_allele AS reference, alt_allele AS variant FROM ref\"\"\",\n",
    "        ref=ref\n",
    "    )\n",
    "    ref_out.write.parquet(ref_out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb9cb2a-df02-4211-89b6-22cd7ef59599",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
