{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jukit_cell_id": "vCt9RCBkvR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import csv\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jukit_cell_id": "jke60eJhBH"
   },
   "source": [
    "# VCF functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jukit_cell_id": "VL3OYp9liS"
   },
   "outputs": [],
   "source": [
    "def get_files_in_directory(directory: str):\n",
    "    # Create an empty list to store the filenames\n",
    "    file_list = []\n",
    "\n",
    "    # Use the os module to get a list of all the files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        # Check that the file is a regular file (not a directory or a special file)\n",
    "        if os.path.isfile(os.path.join(directory, filename)):\n",
    "            # Add the filename to the list\n",
    "            file_list.append(directory+filename)\n",
    "\n",
    "    return sorted(file_list)\n",
    "\n",
    "def get_vcf(vcf_file):\n",
    "\n",
    "    #eventually store header and data lines separately\n",
    "    header = []\n",
    "    vcf_list = []\n",
    "    \n",
    "    with open(vcf_file, newline = '', errors='ignore') as variants:  #open vcf file, take as tab delimited, create huge list that is the vcf file\n",
    "        variant_reader = csv.reader(variants, delimiter='\\t')\n",
    "        for variant in variant_reader:\n",
    "                vcf_list.append(variant)\n",
    "\n",
    "    #list of ALL lines in vcf file\n",
    "    vcf_lines = []\n",
    "\n",
    "    for i in range(len(vcf_list)): #for every line in VCF file\n",
    "\n",
    "        if vcf_list[i][0][0] == \"#\": #if header lines -> header\n",
    "\n",
    "            header.append(str(vcf_list[i][0]))\n",
    "\n",
    "        elif vcf_list[i][0][0] != \"#\": #if data/variant lines -> vcf_lines\n",
    "\n",
    "            vcf_lines.append(vcf_list[i])\n",
    "\n",
    "    return vcf_lines, header\n",
    "\n",
    "\n",
    "def build_ref_df(ref_vcf): #build pandas dataframe from VCF of 'reference'/parental SNPs\n",
    "\n",
    "    snp_lines, snp_header = get_vcf(ref_vcf) #read in VCF, split header from data lines\n",
    "\n",
    "    snp_data = []\n",
    "\n",
    "    for i in range(len(snp_lines)):\n",
    "        df_row = [ snp_lines[i][0], int(snp_lines[i][1]), snp_lines[i][3] ,snp_lines[i][4].split(',')[0] ] #create data rows that are only chromosome, position, reference allele, and variant\n",
    "        snp_data.append(df_row)\n",
    "\n",
    "    ref_df = pd.DataFrame(snp_data, columns=[\"chromosome\", \"position\", \"reference\", \"variant\"]) # make dataframe with given column names\n",
    "\n",
    "    ref_df= ref_df.reset_index(drop=True)\n",
    "\n",
    "    return ref_df\n",
    "\n",
    "\n",
    "def build_GATK_snp_df(snp_vcf): #build pandas dataframe from VCF samples with GATK genotype calls\n",
    "\n",
    "    snp_lines, snp_header = get_vcf(snp_vcf) #create data rows that are only chromosome, position, reference allele, and variant\n",
    "\n",
    "    # our file convention is 'project_name'-'germ_cell_origin'-'sample_number' ... for example, BSP-OR-001 where 'OR' = oocyte-derived recombinant and sample = 001\n",
    "\n",
    "    # WT-G0-001 - WT = wild-type, G0 = the original NDJ progeny (or F1 for the progeny of a male), 001 = ID number\n",
    "\n",
    "    #modify this to fit your file naming convention and check for consistency in later functions\n",
    "    condition = snp_vcf.split(\"/\")[-1].split(\".\")[0].split(\"-\")[0] # per our file naming convention, get genotype/condition\n",
    "\n",
    "    sample_type = snp_vcf.split(\"/\")[-1].split(\".\")[0].split(\"-\")[1] # per our file naming convention, get germ cell of origin\n",
    "\n",
    "    sample = snp_vcf.split(\"/\")[-1].split(\".\")[0].split(\"-\")[2] # per our file naming convention, get sample number\n",
    "\n",
    "    snp_data = []\n",
    "\n",
    "    for i in range(len(snp_lines)):\n",
    "\n",
    "        chrom, position, dot1, ref, alt, qual, dot2, info, annotations, annot_data = snp_lines[i] #split SNP data into variables\n",
    "\n",
    "        annotation_dict = dict(zip(annotations.split(':'), annot_data.split(':'))) # create dictionary for info tags and data\n",
    "\n",
    "        if alt == '<NON_REF>': #replace with more conventional '.' for lack of variant allele\n",
    "            alt = '.'\n",
    "\n",
    "        alt = alt.split(\",\")[0] #take most prevalent alternate allele\n",
    "\n",
    "        if annot_data == '0/0' or annot_data[-1] == './.' or 'DP' not in annotation_dict: \n",
    "        #if weird site with genotype called but NaN or no read counts given...USELESS...skip this line...\n",
    "        \n",
    "            continue\n",
    "\n",
    "\n",
    "        else:\n",
    "\n",
    "            DP = int(annotation_dict['DP']) #get total read number used to call variants\n",
    "\n",
    "            GT = annotation_dict['GT'] #get preliminary genotype call\n",
    "\n",
    "            ref_reads = 0\n",
    "            var_reads = 0\n",
    "\n",
    "            if 'AD' in annotation_dict:\n",
    "                reads = annotation_dict['AD'].split(',') # if Allelic Depth field is present, split into reads per allele\n",
    "\n",
    "                ref_reads = int(reads[0]) #ref reads are always first\n",
    "\n",
    "                if len(reads)>1:\n",
    "                    var_reads = int(reads[1]) # if variant allele present, get read counts\n",
    "\n",
    "            elif 'AD' not in annotation_dict: #unless no variant alleles present, ref reads = total reads\n",
    "\n",
    "                ref_reads = DP\n",
    "\n",
    "\n",
    "\n",
    "            qual = 0 #qual will be zero unless variant is called\n",
    "\n",
    "            if snp_lines[i][4] != '.' and snp_lines[i][4] != '<NON_REF>':\n",
    "\n",
    "                qual = float(snp_lines[i][5]) #get QUAL score\n",
    "\n",
    "            df_row = [ chrom, int(position), ref, alt, qual, int(ref_reads), int(var_reads), DP, GT, sample_type, sample] #make dataframe rows\n",
    "            snp_data.append(df_row)\n",
    "\n",
    "    #make dataframe with given column names matching data in line 116 (df_row)\n",
    "    snp_df = pd.DataFrame(snp_data, columns=[\"chromosome\", \"position\", \"reference\", \"variant\", 'QUAL',\n",
    "                                             'ref_reads', 'variant_reads', 'DP', 'genotype', 'sample_type', 'sample_num'])\n",
    "\n",
    "\n",
    "    snp_df = snp_df[snp_df['genotype']!='./.'] #drop lines with no genotype called\n",
    "    snp_df= snp_df.reset_index(drop=True)\n",
    "    snp_df = snp_df.drop(columns='genotype')\n",
    "\n",
    "    #change names of chromosomes\n",
    "    # chrom_name_dict = {'I_1':\"N2_chrI\", 'II_1':\"N2_chrII\", 'III_1':\"N2_chrIII\", 'IV':\"N2_chrIV\", 'V_1':\"N2_chrV\", 'X_1':\"N2_chrX\"}\n",
    "    # snp_df = snp_df.replace(chrom_name_dict)\n",
    "\n",
    "    return snp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jukit_cell_id": "G0E3HrAOSx"
   },
   "source": [
    "# Parental and Sample VCF dataframe merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jukit_cell_id": "nBkCPoRLSf"
   },
   "outputs": [],
   "source": [
    "def align_vcf_dfs(vcf0, vcf1):\n",
    "    # align dataframes on [chromosome, position] so that all positions are represented\n",
    "    (vcf0, vcf1) = vcf0.set_index(\"position\").align(\n",
    "        vcf1.set_index(\"position\"), fill_value=np.NaN\n",
    "    )\n",
    "    vcfs = []\n",
    "    for i in ((vcf0, vcf1), (vcf1, vcf0)):\n",
    "        vcf = i[0].reset_index()\n",
    "        hom_vcf = i[1].reset_index()\n",
    "        vcf = vcf.fillna(hom_vcf[\"chromosome\"])\n",
    "        vcf = vcf[~vcf.index.duplicated(keep=\"first\")]\n",
    "        mask = vcf[\"position\"].isnull()\n",
    "        vcf.loc[mask, \"position\"] = hom_vcf[\"position\"]\n",
    "        vcfs.append(vcf)\n",
    "    return vcfs\n",
    "\n",
    "def reassign_vcf_reference(vcf0, vcf1):\n",
    "    # this needs to be done for the two parental references and the samples\n",
    "    # all dataframes have to include \"non-variant sites\" (relative to dm6) to get the info for those\n",
    "    # since they could be variants in w1118\n",
    "\n",
    "    # make a df of every position in the dataframes\n",
    "    all_positions = vcf0.join(vcf1, on=['chromosome', 'position'], how='outer')\n",
    "    \n",
    "    vcfs = align_vcf_dfs(vcf0, vcf1)\n",
    "    # this will return a df where vcf1 is the reference sequence and vcf0 is the alternate\n",
    "    # copy other vcf's REF column, some rows should be NaN because vcf1 matches dm6 there\n",
    "    # so for those we will fill with the original REF values\n",
    "    vcfs[0]['REF_x'] = np.where(\n",
    "            vcfs[1].loc[vcfs[1]['ref_reads'].isna()],\n",
    "            vcfs[0].loc['ref_reads'],\n",
    "            vcfs[1]['ref_reads']\n",
    "            )\n",
    "\n",
    "    # if ALT is empty, it should match the original REF at that site\n",
    "    vcfs[0]['ALT_x'] = np.where(\n",
    "            vcfs[0].loc[vcfs[0]['variant_reads'].isna()],\n",
    "            vcfs[1].loc['ref_reads'],\n",
    "            vcfs[0]['variant_reads']\n",
    "            )\n",
    "\n",
    "    vcfs[0].drop(['alt_reads', 'ref_reads'], axis=1)\n",
    "    \n",
    "    # if ALT matches the REF, use . convention\n",
    "    vcfs[0]['alt_reads'] = np.where(\n",
    "            vcfs[0].loc['alt_reads_x' == vcfs[0]['ref_reads_x']],\n",
    "            '.',\n",
    "            vcfs[0]['alt_reads_x']\n",
    "            )\n",
    "\n",
    "    vcfs[0]['ref_reads'] = vcfs[0].loc[:, 'ref_reads_x']\n",
    "    vcfs[0] = vcfs[0].drop(['alt_reads_x', 'ref_reads_x'], axis=1)\n",
    "\n",
    "    # get the QUAL and DP information for the newly added ALT positions from the original dataframe\n",
    "    vcfs[0] = vcfs[0].join(all_positions, on=['chromosome', 'position'], how='left', rsuffix='_right')\n",
    "    vcfs[0] = vcfs[0].drop(['ref_reads_right', 'variant_reads_right', 'QUAL', 'DP'])\n",
    "    vcfs[0] = vcfs[0].rename({'QUAL_right': 'QUAL', 'DP_right': 'DP'})\n",
    "\n",
    "    return vcfs[0]\n",
    "\n",
    "    \n",
    "def merge_sample_ref_dfs(dm6_sample_vcf_df, ref_vcf_df):\n",
    "\n",
    "    # change the sample vcf so the reference matches ref_vcf_df\n",
    "    sample_vcf_df = reassign_vcf_reference(dm6_sample_vcf_df, ref_vcf_df)\n",
    "\n",
    "    ## merge vcf dataframes on [chromosome, position] to retain every position/marker in the reference marker set\n",
    "\n",
    "    df_merge = ref_vcf_df.merge(sample_vcf_df, on=['chromosome','position'], copy=False, how='outer')\n",
    "\n",
    "    ## drop positions/rows that are not in reference marker set\n",
    "\n",
    "    df_merge = df_merge.drop(df_merge.loc[df_merge['reference_x'].isna()].index, inplace=False)\n",
    "\n",
    "    ## drop redundant reference column\n",
    "\n",
    "    df_merge = df_merge.drop(columns=['reference_y'])\n",
    "\n",
    "    ## replace NaN with zeros for marker sites with no variant reads and not called in GATK vcf, set proper empty variant allele\n",
    "\n",
    "    df_merge[['QUAL', \"ref_reads\", \"variant_reads\", \"DP\"]] = df_merge[['QUAL', \"ref_reads\", \"variant_reads\", \"DP\"]].fillna(0)\n",
    "    df_merge['variant_y'] = df_merge['variant_y'].fillna('.')\n",
    "\n",
    "    ## refill missing gamete and sample number values that were NaN\n",
    "\n",
    "    sample_type = df_merge[(df_merge['sample_type']=='G0') | (df_merge['sample_type']=='F1')].sample_type.unique()[0]\n",
    "    sample_num = [x for x in df_merge.sample_num.unique() if isinstance(x, str)][0]\n",
    "\n",
    "    df_merge['sample_type'] = sample_type\n",
    "    df_merge['sample_num'] = sample_num\n",
    "\n",
    "    ## if sample variant is not target marker variant, change to empty variant allele and set variant reads to zero\n",
    "\n",
    "    df_merge['variant_y'] = df_merge.apply(lambda row: '.' if row['variant_x'] != row['variant_y'] else row['variant_y'], axis =1)\n",
    "    df_merge['variant_reads'] = df_merge.apply(lambda row: 0.0 if row['variant_x'] != row['variant_y'] else row['variant_reads'], axis =1)\n",
    "\n",
    "    return df_merge\n",
    "\n",
    "\n",
    "def build_master_dataframe_list(sample_vcf_files: list, alt_parent_file: str, ref_parent_file: str, minimum_dp=50, minimum_var_reads=10, minimum_var_qual=200):\n",
    "\n",
    "    print('building dataframe from reference vcfs...')\n",
    "    ref_parent_vcf_df = build_ref_df(ref_parent_file)\n",
    "    alt_parent_vcf_df = build_ref_df(alt_parent_file)\n",
    "\n",
    "    # anti-join of the parental dataframes to find positions exclusive to each\n",
    "    print('merging reference vcfs into a single reference...')\n",
    "    outer_df = alt_parent_vcf_df.merge(ref_parent_vcf_df, how='outer', indicator=True)\n",
    "    alt_parent_vcf_df_unique = outer_df[(outer_df._merge=='left_only')].drop('_merge', axis=1)\n",
    "    ref_parent_vcf_df_unique = outer_df[(outer_df._merge=='right_only')].drop('_merge', axis=1)\n",
    "    \n",
    "    # edit the vcf dfs so that parent_vcf_df2 is the reference\n",
    "    ref_df = reassign_vcf_reference(alt_parent_vcf_df_unique, ref_parent_vcf_df_unique)\n",
    "\n",
    "    # drop heterozygous reads\n",
    "    ref_df = ref_df.drop(ref_df['genotype'] == '0/1')\n",
    "\n",
    "    print(\"filtering reference sites: requiring minimum of\", minimum_dp, \"total reads per site...\")\n",
    "    ref_df_filt1 = ref_df.drop(ref_df.loc[ (ref_df['DP']<minimum_dp) ].index, inplace=False)\n",
    "\n",
    "    print('filtering reference variant SNPs: require a minimum of', minimum_var_reads, 'variant reads...')\n",
    "    ref_df_filt2 = ref_df_filt1.drop(ref_df_filt1.loc[(ref_df_filt1['variant_reads']<minimum_var_reads)].index, inplace=False)\n",
    "\n",
    "    print('filtering reference SNPs: require a minimum QUAL score of ', minimum_var_qual, '...')\n",
    "    ref_df_filt3 = ref_df_filt2.drop(ref_df_filt2.loc[(ref_df_filt2['QUAL']<minimum_var_qual)].index, inplace=False)\n",
    "\n",
    "    sample_vcf_merges = []\n",
    "\n",
    "    print('building dataframes from each sample vcf and merging with reference vcf...')\n",
    "\n",
    "    for i in range(len(sample_vcf_files)):\n",
    "\n",
    "        merge_df = merge_sample_ref_dfs(build_GATK_snp_df(sample_vcf_files[i]), ref_df_filt3) \n",
    "\n",
    "        sample_vcf_merges.append(merge_df)\n",
    "\n",
    "        print(sample_vcf_files[i].split('/')[-1], 'complete...')\n",
    "\n",
    "    print('building master dataframe of chromosomes...')\n",
    "    master_recombinant_df = pd.concat(sample_vcf_merges, ignore_index=True)\n",
    "    \n",
    "    ##uncomment below to make use of filters for minimum read depth, minimum variant reads, and minimum QUAL score\n",
    "\n",
    "#     print(\"filtering all sites: requiring minimum of\", minimum_dp, \"total reads per site...\")\n",
    "#     master_recombinant_df_filt1 = master_recombinant_df.drop(master_recombinant_df.loc[ (master_recombinant_df['DP']<minimum_dp) ].index, inplace=False)\n",
    "\n",
    "#     print('filtering variant SNPs: require a minimum of', minimum_var_reads, 'variant reads...')\n",
    "#     master_recombinant_df_filt2 = master_recombinant_df_filt1.drop(master_recombinant_df_filt1.loc[(master_recombinant_df_filt1['Code']=='CB4856') & (master_recombinant_df_filt1['variant_reads']<minimum_var_reads)].index, inplace=False)\n",
    "\n",
    "#     print('filtering variant SNPs: require a minimum QUAL score of ', minimum_var_qual, '...')\n",
    "#     master_recombinant_df_filt3 = master_recombinant_df_filt2.drop(master_recombinant_df_filt2.loc[(master_recombinant_df_filt2['Code']=='CB4856') & (master_recombinant_df_filt2['QUAL']<minimum_var_qual)].index, inplace=False)\n",
    "\n",
    "    #master_recombinant_df = master_recombinant_df_filt3\n",
    "\n",
    "    master_recombinant_df = master_recombinant_df.drop(columns=['variant_y'])\n",
    "    master_recombinant_df = master_recombinant_df.rename(columns={'reference_x':'reference', 'variant_x':'variant'})\n",
    "    master_recombinant_df = master_recombinant_df.astype({'ref_reads':int, 'variant_reads':int})\n",
    "    master_recombinant_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print(\"done!\")\n",
    "\n",
    "    return master_recombinant_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jukit_cell_id": "AqeBdEzD6Z"
   },
   "source": [
    "# TIGER functions\n",
    "\n",
    "* adapted methods from Rowan et. al. : https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4349092/\n",
    "* original TIGER scripts found at: https://github.com/betharowan/TIGER_Scripts-for-distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jukit_cell_id": "DfE1Hpl8UB"
   },
   "outputs": [],
   "source": [
    "def get_TIGER_inputs_and_mkdirs(new_tiger_directory: str):\n",
    "\n",
    "    os.mkdir(new_tiger_directory)\n",
    "\n",
    "    # Create an empty list to store the filenames\n",
    "    file_list = []\n",
    "\n",
    "    # Use the os module to get a list of all the files in the current directory\n",
    "\n",
    "    for filename in os.listdir(os.getcwd()):\n",
    "\n",
    "        if os.path.isfile(os.path.join(os.getcwd(), filename)):\n",
    "\n",
    "            if filename.split('.')[-2] == \"tiger_input\":\n",
    "\n",
    "                os.mkdir(new_tiger_directory+'/'+filename.split('.')[0])\n",
    "\n",
    "                os.replace( os.getcwd()+'/'+filename , new_tiger_directory+'/'+filename.split('.')[0]+'/'+filename )\n",
    "\n",
    "def create_TIGER_inputs_and_directory(master_dataframe, new_project_dir: str):\n",
    "\n",
    "    if new_project_dir not in os.listdir():\n",
    "\n",
    "        for sample_type in master_dataframe.sample_type.unique():\n",
    "\n",
    "            for sample_num in master_dataframe.sample_num.unique(): #subset master by individual samples...\n",
    "    \n",
    "                filename = str(sample_type) + \"_\" + str(sample_num)+\".tiger_input.txt\"\n",
    "    \n",
    "                df = master_dataframe[(master_dataframe['sample_type']==sample_type) & (master_dataframe['sample_num']==sample_num)]\n",
    "    \n",
    "    \n",
    "                df['tiger_chrom'] = df.apply(lambda row: tiger_chrom_name_dict[row['chromosome']], axis=1)\n",
    "    \n",
    "                df.to_csv(filename, sep=\"\\t\", header=False, index=False, columns = ['tiger_chrom', 'position', 'reference', 'ref_reads', 'variant', 'variant_reads'])\n",
    "    \n",
    "            get_TIGER_inputs_and_mkdirs(new_tiger_directory=new_project_dir)\n",
    "    \n",
    "        else:\n",
    "            print(\"No TIGER input or folders made. This project folder already exists...\")\n",
    "\n",
    "def run_TIGER_pipeline(master_dataframe, project_dir: str):\n",
    "\n",
    "    #try to make TIGER inputs\n",
    "\n",
    "\n",
    "    if project_dir not in os.listdir(): # if project folder not made...\n",
    "\n",
    "        print('making TIGER inputs...')\n",
    "        create_TIGER_inputs_and_directory(master_dataframe, new_project_dir=project_dir) #make inputs, project directory, and sample sub-directories\n",
    "\n",
    "        for sample_dir in os.listdir(project_dir):\n",
    "\n",
    "            sample_folder = os.getcwd()+'/'+project_dir+'/'+sample_dir+'/'\n",
    "\n",
    "            if sample_dir != '.DS_Store':\n",
    "\n",
    "                if sample_dir+'.tiger_input.txt' in os.listdir(sample_folder) and sample_dir+'.CO_estimates.txt' not in os.listdir(sample_folder): #if folder has input but missing this TIGER output, do TIGER\n",
    "\n",
    "                    subprocess.run(['sh', 'run_TIGER.sh', os.getcwd()+'/'+project_dir+'/'+sample_dir+'/', sample_dir])\n",
    "\n",
    "                elif sample_dir+'.tiger_input.txt' in os.listdir(sample_folder) and sample_dir+'.CO_estimates.txt' in os.listdir(sample_folder): #if folder has input and has this TIGER output, skip...\n",
    "                    print(sample_dir, 'already processed...')\n",
    "                    continue\n",
    "\n",
    "                elif sample_dir+'.tiger_input.txt' not in os.listdir(sample_folder): #warn if input not in folder for some reason\n",
    "                    print(sample_dir, 'needs TIGER input...')\n",
    "\n",
    "\n",
    "\n",
    "    elif project_dir in os.listdir(): #if project directory alredy exists\n",
    "        for sample_dir in os.listdir(project_dir): #for every sample directory, run tiger\n",
    "\n",
    "            if sample_dir != '.DS_Store': #skip mac's DS_store file...useless\n",
    "\n",
    "                sample_folder = os.getcwd()+'/'+project_dir+'/'+sample_dir+'/'\n",
    "\n",
    "                if sample_dir+'.tiger_input.txt' in os.listdir(sample_folder) and sample_dir+'.CO_estimates.txt' not in os.listdir(sample_folder): #if folder has input but missing this TIGER output, do TIGER\n",
    "\n",
    "                    subprocess.run(['sh', 'run_TIGER.sh', os.getcwd()+'/'+project_dir+'/'+sample_dir+'/', sample_dir])\n",
    "\n",
    "                elif sample_dir+'.tiger_input.txt' in os.listdir(sample_folder) and sample_dir+'.CO_estimates.txt' in os.listdir(sample_folder): #if folder has input and has this TIGER output, skip...\n",
    "                    print(sample_dir, 'already processed...')\n",
    "                    continue\n",
    "\n",
    "                elif sample_dir+'.tiger_input.txt' not in os.listdir(sample_folder): #warn if input not in folder for some reason\n",
    "                    print(sample_dir, 'needs TIGER input...')\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jukit_cell_id": "EwBATLXf52"
   },
   "source": [
    "# Functions for processing TIGER outputs and creating new dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jukit_cell_id": "e7ikkElxba"
   },
   "source": [
    "## Processing TIGER outputs to create SNP/marker dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jukit_cell_id": "LHFFozMf8T"
   },
   "outputs": [],
   "source": [
    "def get_TIGER_files(project_dir: str, file_pattern: str):\n",
    "\n",
    "    files = []\n",
    "\n",
    "    for folder in os.listdir(project_dir):\n",
    "\n",
    "        if folder[0] != '.':\n",
    "\n",
    "            files.append(project_dir+'/'+folder+\"/\"+folder+file_pattern)\n",
    "\n",
    "    return files\n",
    "\n",
    "\n",
    "def create_TIGER_master_df(project_dir: str, file_pattern: str):\n",
    "    #this dataframe this function makes is relatively large because it gives you genotype and HMM state calls for every SNP marker across each chromosome for each sample\n",
    "    \n",
    "    #This is closest to the 'raw' data that TIGER outputs, but this format is mostly useful for plotting and visualization of individual chromosomes for manual inspection of SNPs and crossover calls\n",
    "\n",
    "    #get list of file names for all TIGER outputs\n",
    "    files_list = get_TIGER_files(project_dir, file_pattern)\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for file in files_list:\n",
    "\n",
    "        #make dataframe; adjust column names if you end up using different TIGER output files than I did (file_pattern='.CO_estimates.txt')\n",
    "        df = pd.read_csv(file, sep=\"\\t\", header=None, names = [\"Sample\", \"chromosome\", \"position\", \"base_geno\", \"hmm_state1\", \"hmm_state2\",\n",
    "                                                   \"reference\", \"ref_reads\", \"variant\", \"var_reads\"])\n",
    "\n",
    "        #This dict will convert parental arabidopsis genotype calls (Col and Ler) to C. elegans genotype calls (N2 is Bristol WT, CB4856 is Hawaiian WT)\n",
    "        #Replace these values with your preferred genotype labels as needed\n",
    "        ref_parental_genotype = \"w1118\"\n",
    "        alt_parental_genotype = \"Oregon RM\"\n",
    "        genotype_dict = {'CC':ref_parental_genotype, \"CL\":\"het\", \"LL\":alt_parental_genotype, \"CU\":\"u\"+ref_parental_genotype, \"LU\":'u'+alt_parental_genotype, \"UN\":\"unknown\", '?':\"?\"}\n",
    "        df = df.replace(genotype_dict)\n",
    "        \n",
    "        #for our specific cross scheme to ID crossovers, heterozygous and homozygous CB4856 calls are should all be considered CB4856\n",
    "        df['hmm_state1'] = df['hmm_state1'].replace({'het':alt_parental_genotype})\n",
    "        \n",
    "        #add a unique identifier for each chromosome\n",
    "        #example... BSP-OR-001-1 = (project)-(gamete)-(sample number)-(chromosome)\n",
    "        df['chrom_id'] = df.apply(lambda row: row['Sample']+\"-\"+str(row['chromosome']), axis=1)\n",
    "\n",
    "        print(file, 'done')\n",
    "        data.append(df)\n",
    "\n",
    "    # make dataframe from all samples in data list\n",
    "    data = pd.concat(data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_master_df(master_df_file):\n",
    "    pd.read_parquet(master_df_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jukit_cell_id": "Ea5rl4cbQk"
   },
   "source": [
    "## Processing TIGER outputs to create intervals dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jukit_cell_id": "AqR1PlEPlb"
   },
   "outputs": [],
   "source": [
    "def make_transition_intervals_df(master_df):\n",
    "    \n",
    "    #These new transition intervals are the potential crossover intervals that can be classified later as crossover or 'not crossover'\n",
    "\n",
    "    new_dfs = []\n",
    "\n",
    "    for chrom_id in master_df.chrom_id.unique():\n",
    "\n",
    "        df = master_df[master_df['chrom_id']==chrom_id]\n",
    "\n",
    "        starts = list(df.start)\n",
    "        stops = list(df.stop)\n",
    "        states = list(df.hmm_state)\n",
    "\n",
    "        new_intervals = []\n",
    "\n",
    "        for i in range(len(stops)-1):\n",
    "\n",
    "            if states[i] != states[i+1]:\n",
    "\n",
    "                row = [ df['sample'].unique()[0], df['chromosome'].unique()[0], stops[i], starts[i+1], 'transition', df['chrom_id'].unique()[0], (starts[i+1]-stops[i]) ]\n",
    "\n",
    "                new_intervals.append(row)\n",
    "\n",
    "        new_intervals_df = pd.DataFrame(new_intervals, columns=df.columns)\n",
    "\n",
    "\n",
    "        full_intervals_df = pd.concat([df, new_intervals_df]).sort_values(by='start').reset_index(drop=True)\n",
    "\n",
    "        new_dfs.append(full_intervals_df)\n",
    "\n",
    "\n",
    "\n",
    "    return pd.concat(new_dfs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_state_intervals_df(project_dir: str, file_pattern: str):\n",
    "    #This will create a much smaller dataframe than the marker dataframe above by collapsing runs of the same HMM state into intervals and creating a new row for a 2-marker transition interval between HMM states\n",
    "\n",
    "    #for our specific cross scheme to ID crossovers, heterozygous and homozygous CB4856 calls are should all be considered CB4856\n",
    "    genotype_dict = {'CC':ref_parental_genotype, \"CL\":alt_parental_genotype, \"LL\":alt_parental_genotype, \"CU\":\"u\"+ref_parental_genotype, \"LU\":'u'+alt_parental_genotype, \"UN\":\"unknown\", '?':\"?\"}\n",
    "\n",
    "    files_list = get_TIGER_files(project_dir=project_dir, file_pattern=file_pattern)\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for file in files_list:\n",
    "\n",
    "        #create df given columns in files using file_pattern='.CO_estimates.breaks.txt' in TIGER outputs\n",
    "        df = pd.read_csv(file, sep=\"\\t\", header=None, names = [\"sample\", \"chromosome\", \"start\", \"stop\", \"hmm_state\"])\n",
    "\n",
    "        df['hmm_state'] = df.apply(lambda row: genotype_dict[(row['hmm_state'])], axis=1)\n",
    "        df['chrom_id'] = df.apply(lambda row: row['sample']+\"-\"+str(row['chromosome']), axis=1)\n",
    "        df['length'] = df.apply(lambda row: row['stop']-row['start']+1, axis=1)\n",
    "#         df['gamete'] = df['sample'].unique()[0].split(\"-\")[1]\n",
    "\n",
    "        data.append(df)\n",
    "\n",
    "\n",
    "    master_df = pd.concat(data)\n",
    "\n",
    "    master_df = make_transition_intervals_df(master_df)\n",
    "\n",
    "    master_df = master_df.sort_values(by=['chrom_id', 'chromosome', 'start']).reset_index(drop=True)\n",
    "\n",
    "    return master_df\n",
    "\n",
    "\n",
    "def get_marker_counts_per_interval(master_bases_df, master_intervals_df):\n",
    "    \n",
    "    #This is useful to retain marker density information for each interval, which is needed for later maths and classification of crossover intervals\n",
    "\n",
    "    new_dfs = []\n",
    "\n",
    "    for chrom_id in master_bases_df.chrom_id.unique():\n",
    "\n",
    "        bases_df = master_bases_df[master_bases_df['chrom_id']==chrom_id]\n",
    "        intervals_df = master_intervals_df[master_intervals_df['chrom_id']==chrom_id]\n",
    "\n",
    "        starts = list(intervals_df.start)\n",
    "        stops = list(intervals_df.stop)\n",
    "\n",
    "        counts = []\n",
    "\n",
    "        for i in range(len(starts)):\n",
    "\n",
    "            counts.append(len(bases_df[(bases_df['position']>=starts[i]) & (bases_df['position']<=stops[i]) ]))\n",
    "\n",
    "        intervals_df['marker_counts'] = counts\n",
    "\n",
    "        new_dfs.append(intervals_df)\n",
    "\n",
    "    markers_counted_df = pd.concat(new_dfs)\n",
    "\n",
    "    return markers_counted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jukit_cell_id": "8CjgA4dhPv"
   },
   "source": [
    "# Ubiquitious items and file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jukit_cell_id": "Wet8Q2Tuds"
   },
   "outputs": [],
   "source": [
    "work_dir = os.getcwd()\n",
    "\n",
    "#dictionary of chromosome names and corresponding lengths, change these as needed!\n",
    "n2_fasta_names = ['chr2L', 'chr2R', 'chr3L', 'chr3R', 'chr4', 'chrX']\n",
    "\n",
    "# TODO: change these\n",
    "n2_chrom_lengths = [15114068, 15311845, 13819453, 17493838, 20953657, 17739129]\n",
    "\n",
    "tiger_chrom_len_dict = dict(zip([1,2,3,4,5,6], n2_chrom_lengths))\n",
    "tiger_chrom_name_dict = dict(zip(n2_fasta_names, [1,2,3,4,5,6]))\n",
    "# tiger_chrom_len_dict = dict(zip([1,2,3,4,5,6], n2_chrom_lengths))\n",
    "# tiger_chrom_name_dict = dict(zip(n2_fasta_names, [1,2,3,4,5,6]))\n",
    "\n",
    "# ref_parental_genotype = 'N2' #change this to your reference genotype\n",
    "# alt_parental_genotype = 'CB4856' #change this to your alternate parental genotype\n",
    "\n",
    "ref_parental_genotype = 'w1118' #change this to your reference genotype\n",
    "alt_parental_genotype = 'oregonr' #change this to your alternate parental genotype\n",
    "\n",
    "# sample_vcfs_path = '/Users/zac/Desktop/VCF_to_TIGER/sample_vcf/'\n",
    "# cb_ref_vcf = '/Users/zac/Desktop/VCF_to_TIGER/reference_vcf/CB.aligned.to.N2.SNPs.noHet.no-repeats.vcf'\n",
    "sample_vcfs_path = os.path.join(work_dir, \"samples/\")\n",
    "ref_vcf = os.path.join(work_dir, \"references/w1118_snps.vcf\")\n",
    "alt_vcf = os.path.join(work_dir, \"references/oregonr_snps.vcf\")\n",
    "\n",
    "sample_vcfs = get_files_in_directory(sample_vcfs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jukit_cell_id": "6ZGUdxXHBL"
   },
   "source": [
    "# Run Pipeline: VCF processing > TIGER HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jukit_cell_id": "8dsyUCiYyH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building dataframe from reference vcfs...\n"
     ]
    }
   ],
   "source": [
    "#VCF Processing and save\n",
    "master_df_file = \"../../data/parquets/progeny.parquet2\"\n",
    "# vcf_master_df = build_master_dataframe_list(sample_vcf_files=sample_vcfs, alt_parent_file=alt_vcf, ref_parent_file=ref_vcf)\n",
    "vcf_master_df = pd.read_parquet(master_df)\n",
    "#Save the dataframe as a pickle file\n",
    "save_vcf_data_as = \"test_vcfs.pickle.gzip\"\n",
    "vcf_master_df.to_pickle(save_vcf_data_as, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jukit_cell_id": "aq1xLrYz25"
   },
   "outputs": [],
   "source": [
    "#run TIGER scripts to get HMM outputs\n",
    "run_TIGER_pipeline(vcf_master_df, project_dir='TIGER_test_output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jukit_cell_id": "SeG8LIZjhR"
   },
   "outputs": [],
   "source": [
    "#generate TIGER marker and interval dataframes\n",
    "tiger_marker_df = create_TIGER_master_df(project_dir='TIGER_test_output', file_pattern='.CO_estimates.txt')\n",
    "tiger_marker_df.to_pickle('TIGER_hmm_states.all_markers.pickle.gzip', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jukit_cell_id": "TscRHFvSLv"
   },
   "outputs": [],
   "source": [
    "tiger_pre_intervals = create_state_intervals_df(project_dir='TIGER_test_output', file_pattern='.CO_estimates.breaks.txt')\n",
    "tiger_intervals = get_marker_counts_per_interval(tiger_marker_df, tiger_pre_intervals)\n",
    "\n",
    "tiger_intervals.to_pickle('TIGER_hmm_intervals.pickle.gzip', compression='gzip')\n",
    "tiger_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
